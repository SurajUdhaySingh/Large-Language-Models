{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b066f7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_26652\\166369440.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"nanollama_weights.pth\"))\n"
     ]
    }
   ],
   "source": [
    "from transformer import LlamaLMHeadModel, Config\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn\n",
    "\n",
    "config = Config()\n",
    "model = LlamaLMHeadModel(config)\n",
    "model.load_state_dict(torch.load(\"nanollama_weights.pth\"))\n",
    "model = model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24dfc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_autoregressive(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 50,\n",
    "    eos_token_id: int = 3,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensor=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = model(input_ids)  # logits: [1, seq_len, vocab_size]\n",
    "            logits = outputs[:, -1, :] / temperature  # take last token's logits\n",
    "            if top_k is not None:\n",
    "                top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
    "                probs = torch.nn.functional.softmax(top_k_logits, dim=-1)\n",
    "                next_token = top_k_indices[0, torch.multinomial(probs, 1)]\n",
    "            else:\n",
    "                probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)\n",
    "\n",
    "            # Squeeze the next_token and concatenate with input_ids\n",
    "            next_token = next_token.squeeze(1)  # Remove extra dimension\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(1)], dim=1)  # Concatenate\n",
    "\n",
    "            if next_token.item() == eos_token_id:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(input_ids[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f543fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> The future of artificial intelligence is</s> to the largest in the Spanish Empire of life. Sorand of the the Soviet Union of the Soviet Union were also: for example, such as the end of St√©-Da-y-stor, his reigned a small-t-fr-being, but there were also influenced by the first used to have a similar number of the French Revolution in his army, French-parting to, for his brother of the US and for his army; however, who\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import NanoLlamaTokenizer\n",
    "\n",
    "tokenizer = NanoLlamaTokenizer(\"nanollama_tokenizer.model\")\n",
    "\n",
    "generated = generate_autoregressive(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=\"The future of artificial intelligence is\",\n",
    "    max_new_tokens=100,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b067e620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Once upon a time</s>. It has been an important part of the to direct, but also most notably in the same for most prominent of the city in their own very popular in the French Revolution, the early in the early 20th century BC, the the term is believed to holder a small parts of the development of the social factors of the \"ID\" to be many of the the Sed of the largest\" of the American state in the 21st century BC\" of the social systems was in a high\n"
     ]
    }
   ],
   "source": [
    "generated = generate_autoregressive(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=\"Once upon a time \",\n",
    "    max_new_tokens=100,\n",
    "    top_k=40\n",
    ")\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecdd56c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
