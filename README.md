# Language Modeling and Sequence Transduction with Modern LLMs

This repository is dedicated to the exploration and implementation of **language modeling** and **sequence transduction**, with a primary focus on **modern Large Language Models (LLMs)** based on the **attention mechanism** and **auto-regressive architectures**.

## Overview

In recent years, transformer-based models have gained traction because of being scalable, allowing training of models on an unprecedented scale, far surpassing RNN based sequence models. This project aims to:

- Develop and experiment with attention-based models from scratch using PyTorch.
- Investigate the role of auto-regression in generating coherent and contextually accurate sequences.
- Explore architectural choices such as rotary positional embeddings, residual connections, normalization, and dropout.
- Support both theoretical understanding and practical application.

## Core Features

- Custom implementation of attention mechanisms (including Rotary Positional Embeddings).
- Multi-layer transformer blocks with configurable hyperparameters.
- RMS Layer Normalization and flexible MLP structures.
- Training-ready language model head with auto-regressive loss handling.
- Lightweight, modular PyTorch codebase.

## Technologies Used

- `Python 3.x` - my favourite tool!
- `PyTorch`
- Several Python libraries, including the ones included in base language.
- Modern architectural patterns inspired by models such as LLaMA

## File Structure
- Each folder contains an entirely different project, there's only one, for now.

